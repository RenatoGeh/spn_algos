\documentclass{amsart}

% biber

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{thmtools,thm-restate}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage[backend=biber,url=true,doi=true,eprint=false,style=numeric]{biblatex}
\usepackage{enumitem}
\usepackage[justification=centering,singlelinecheck=false]{caption}
\usepackage{indentfirst}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage[x11names, rgb]{xcolor}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{linegoal}
\usepackage{csquotes}
\usetikzlibrary{snakes,arrows,shapes}

\addbibresource{references.bib}

\makeatletter
\def\subsection{\@startsection{subsection}{3}%
  \z@{.5\linespacing\@plus.7\linespacing}{.1\linespacing}%
  {\normalfont}}
\makeatother

\makeatletter
\patchcmd{\@setauthors}{\MakeUppercase}{}{}{}
\makeatother

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\Val}{\text{Val}}
\DeclareMathOperator*{\Ch}{\text{Ch}}
\DeclareMathOperator*{\Pa}{\text{Pa}}
\DeclareMathOperator*{\Sc}{\text{Sc}}
\newcommand{\ov}{\overline}
\newcommand{\region}{\mathcal}

\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}

\newcommand{\algorithmautorefname}{Algorithm}
\algrenewcommand\algorithmicrequire{\textbf{Input}}
\algrenewcommand\algorithmicensure{\textbf{Output}}
\algrenewcomment[1]{\hspace{0.25cm}\(\triangleright\) #1}
\algnewcommand{\LineComment}[1]{\State\,\(\triangleright\) #1}

\captionsetup[table]{labelsep=space}

\theoremstyle{plain}

\newcounter{dummy-def}\numberwithin{dummy-def}{section}
\newtheorem{definition}[dummy-def]{Definition}
\newcounter{dummy-thm}\numberwithin{dummy-thm}{section}
\newtheorem{theorem}[dummy-thm]{Theorem}
\newcounter{dummy-prop}\numberwithin{dummy-prop}{section}
\newtheorem{proposition}[dummy-prop]{Proposition}
\newcounter{dummy-corollary}\numberwithin{dummy-corollary}{section}
\newtheorem{corollary}[dummy-corollary]{Corollary}
\newcounter{dummy-lemma}\numberwithin{dummy-lemma}{section}
\newtheorem{lemma}[dummy-lemma]{Lemma}
\newcounter{dummy-ex}\numberwithin{dummy-ex}{section}
\newtheorem{exercise}[dummy-ex]{Exercise}
\newcounter{dummy-eg}\numberwithin{dummy-eg}{section}
\newtheorem{example}[dummy-eg]{Example}

\numberwithin{equation}{section}

\newcommand{\set}[1]{\mathbf{#1}}
\newcommand{\pr}{\mathbb{P}}
\newcommand{\eps}{\varepsilon}
\renewcommand{\implies}{\Rightarrow}

\newcommand{\bigo}{\mathcal{O}}

\setlength{\parskip}{1em}

\lstset{frameround=fttt,
	numbers=left,
	breaklines=true,
	keywordstyle=\bfseries,
	basicstyle=\ttfamily,
}

\newcommand{\code}[1]{\lstinline[mathescape=true]{#1}}
\newcommand{\mcode}[1]{\lstinline[mathescape]!#1!}
\newcommand{\dset}[1]{\mathcal{#1}}
\newcommand{\ddspn}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\iddspn}[2]{\partial#1/\partial#2}

\title{%
  \noindent\rule{13cm}{1.0pt}\\
  \vspace{0.2cm}
  The Poon-Domingos Sum-Product Network Learning Algorithm for Image Completion and Classification
  \noindent\rule{13cm}{0.8pt}
}
\xdef\shorttitle{The Poon-Domingos Algorithm}
\author[]{\normalsize\textbf{Renato Lui Geh}\\\small Computer Science\\Institute of Mathematics
  and Statistics\\University of SÃ£o Paulo\\\texttt{renatolg@ime.usp.br}}

\begin{document}

\begin{abstract}
  In this document we describe the Poon-Domingos~\cite{poon-domingos} learning algorithm for image
  classification and completion.
  \vspace*{-3.5em}
\end{abstract}

\maketitle

\section{Introduction}

Sum-Product Networks (SPNs) are Probabilistic Graphical Models (PGMs) that model a tractable
probability distribution. Inference is done through a top-down and bottom-up on the graph at most,
meaning inference is linear wrt its edges. Recent work on SPNs have shown promising results in
various fields, such as protein folding~\cite{rec-dec-non-convex}, speech
modelling~\cite{model-speech}, image classification and
completion~\cite{gens-domingos,poon-domingos,clustering}, activity recognition~\cite{activity} and
natural language~\cite{nat-lang}. Theoretical also show great results, such as SPNs deep structure
representational power~\cite{shallow-vs-deep} and a generalization of SPNs to any semiring with
disjoint product scopes~\cite{sp-theorem}.

Although SPNs have shown great promise, current literature on learning algorithms for SPNs is often
vague and incomplete. This document is part of a series of papers in which we plan on clarifying
and detailing each learning algorithm. In many cases when the original articles omit certain
details, we either look for them in the original code (if it is publicly available) or make
assumptions based on other related articles. When this is the case, we explicitly mention our
assumptions. We restrict our papers to mostly theoretical findings on the learning algorithms, and
do not comment on implementation details. However, we give a detailed description on the algorithms
nonetheless. Throughout this paper, we assume the reader has the necessary background on SPNs and
has read the original paper~\cite{poon-domingos}.

In this paper in particular, we focus on the Poon-Domingos dense structure for image classification
and completion~\cite{poon-domingos}. We first define a few necessary terms that we shall use
throughout the paper. Next we give an overview of the dense structure and what our objectives are.
We then explain the interactions and properties of each component of the structure. Finally we show
how to learn the weights through hard EM and hard gradient descent.

\section{Definitions}

In this section we define the basic blocks for constructing the Poon-Domingos dense architecture.

\begin{definition}[Region]
  A Region $R$ is an ordered set of $m$ (where $m$ is constant) SPN nodes. We shall use the
  following notation for $R$
  \begin{equation*}
    R = (r_1,r_2,\ldots,r_m)
  \end{equation*}
  where $r_i$ is the $i$-th node of $R$.
\end{definition}

We say $r_i$ is the max node of $R$ when $r_i\geq r_j$, with $1\leq j\leq m$. We shall denote such a
max node of $R$ as $r^*$. We will use letters $R$, $S$ and $T$ for regions, all with similar
notations with regard to their nodes.

Our goal is to map a region to a rectangular region of an image, with each unique region
representing a unique rectangular region of the image. We call $\hat{R}$ the region that
encompasses the entire image. The region $\hat{R}$ is an exception to the rule in that it only
contains one sum node, which turns out to be the root node of the SPN\@.

The scope of a region $R$ is the same as the scope of each $r_i$, and is equal to the scope of the
rectangular region of the image (i.e.\ the set of pixels).

\begin{definition}[Unit Region]
  A unit region is a region that contains only univariate probability distributions as nodes (e.g.
  a gaussian).
\end{definition}

A unit region, in the scope of our image application, represents a pixel. Each node $u_i$ in a unit
region $U$ is the $i$-th quantile of the distribution of pixel $U$.

\begin{definition}[Decomposition]
  A Decomposition $D$ is a tuple
  \begin{equation*}
    D = (r, s, t, \pi)
  \end{equation*}
  where $r$, $s$ and $t$ are sum nodes belonging to regions $R$ and $S$ respectively and $\pi$ is a
  product node. A decomposition $D$ exists in an SPN $\mathbf{S}$ if $r$, $s$, $t$, and $\pi$ are
  nodes of $\mathbf{S}$ and $r\in\Pa(\pi)$, $s,t\in\Ch(\pi)$.
\end{definition}

We say a region $S$ is a subregion of $R$ if there exists a possible decomposition $D=(r,s,t,\pi)$,
where $r\in R$ and $s\in S$. $R$ is then the parent region of $S$. In our application, the
subregion $S$ would be a horizontal or vertical partition of region $R$. So a decomposition $D$ of
subregions $R$, $S$ and $T$ would be taking the parent region $R$ and finding an axis-aligned
partitioning of $R$ into two subregions $S$ and $T$. The product node $\pi$ would be the
decomposition of scope of $R$.

A max decomposition is a decomposition $D^*$ where $D^*=(r^*,s^*,t^*,\pi)$. In our learning
algorithm, we wish to find max decompositions for each training instance $I$, since this means we
get the best possible partitioning, maximizing the likelihood.

We only consider valid decompositions. That is, if $\Sc(S)\cup\Sc(T)=\Sc(R)$. If this condition is
not true, then the underlying SPN would not be valid, since it would break consistency.

A resolution $\rho$ is a positive integer representing the level of detail of a decomposition. We
call a region $R$ a fine region if all possible decompositions of the parent region $R$ have
$\rho=1$. That is, we are allowed to partition $R$ into every possible axis-aligned rectangular
region. A region $R$ is a coarse region when we can only decompose $R$ into at most $\rho$-sized
subregions. In other words, the smallest possible size of $S$ in a possible partition of $R$ with
$rho>1$ is $\rho\times\rho$. Coarse regions are rectangular regions with size $(x\cdot\rho)\times
(y\cdot\rho)$, with $x\geq 1$ and $y\geq 1$, but $\neg((x=1)\wedge(y=1))$; fine regions are all
other rectangular regions smaller then coarse regions with the exception of unit regions. Both
coarse and fine regions have sum nodes as their internal nodes.

\section{Overview}

The idea of the Poon-Domingos dense structure is to generate an SPN that finds the maximum
decompositions at each instance of the dataset. In the original paper, Poon and Domingos imply that
the process of constructing the dense architecture is separate to the learning itself. However, an
inspection of their code proved this to be false. In fact, learning is intricately coupled to
generating the dense structure. A general idea of their learning algorithm is as follows.

Create every possible region of the image (i.e.\ every rectangular region of the image). For each
unit region, create each $i$-th leaf node as a gaussian of the $i$-th quantile of the pixel. For
each instance $I$ of the dataset, find all max decompositions for each region wrt $I$ and store
them. For all these decompositions, if they already exist, increase weights of the corresponding
edges; else create a new decomposition with weights zeroed. Repeat this maximization until
convergence. Once converged, remove all parentless non-root nodes or any component of the graph
that cannot be reached by the root node.

One might conjecture whether one could simply create all possible decompositions instead of
maximizing, and then running a parameter learning algorithm. We tested this conjecture through hard
gradient descent, and found out that, due to the nature of MAP inference and the number of
decompositions, the Viterbi-style learning algorithm always took the shortest path, giving
preference to small, long regions. This caused the image to become an average of all training
instances with small regions colored oddly.

Running the Poon-Domingos version, where we maximize decompositions at each instance of the dataset
yielded much better results.

\section{Structure}


\section{Acknowledgements}

This document was funded by CNPq, grant \#800585/2016--0.

%--------------------------------------------------------------------------------------------------

\printbibliography[]

\end{document}
